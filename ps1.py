# -*- coding: utf-8 -*-
"""RAG PS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XzNC-BD6XFwfcQ134Whmpg41UQloVDIA

Log in to Huggingface CLI and install dependencies
"""

!huggingface-cli login

!pip install transformers==4.36.0 timm peft

# test.py
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

"""Load in MiniCPMV V2 to describe the image"""

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True, torch_dtype=torch.bfloat16)
model = model.to(device='cuda', dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True)
model.eval()

import requests
from io import BytesIO

image_link = 'https://m.media-amazon.com/images/I/51PfGSROVWL._AC_UY1100_.jpg'

image_response = requests.get(image_link)
image = Image.open(BytesIO(image_response.content)).convert('RGB')

# Prepare the question
question = "Caption the image in 10 words"
msgs = [{'role': 'user', 'content': question}]

desc, _, _ = model.chat(
    image=image,
    msgs=msgs,
    context=None,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.7
)
print(desc)

"""Load in Qwen2.5-1.5B-Instruct to get search query"""

# Load model directly
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer_qwen2 = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
model2 = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct", device_map = 0)

def main_query_answering(query, max_tokens=100):
    """
    Answer the main query using Meta-Llama-3.2-3B and the retrieved documents.
    """
    # Prepare the prompt using the main query and the relevant shortened documents
    # Assuming 'body' key holds the document text in retrieved_documents
    prompt = f"You are an engine used to generate web search queries. Omit information you dont need, depending on what the user wants. Generate and return the correct search query. {query} \n. The web search query is (in 5 words or less):"

    # Tokenize input for LLM
    inputs = tokenizer_qwen2(prompt, return_tensors="pt").to("cuda")

    # Generate answer for the main query with reduced token output
    outputs = model2.generate(**inputs, max_new_tokens=max_tokens, temperature=0.5)

    # Decode and extract answer
    answer = tokenizer_qwen2.decode(outputs[0], skip_special_tokens=True).strip()

    return answer

query = "Show me tshirts with the same design"

response = main_query_answering(f"The user wants: {query}. The given information is: {desc}.")
search_query = response.split('"')[1]
print(search_query)

"""Use Open Source Duckduckgo API to return search results"""

!pip install duckduckgo_search

from duckduckgo_search import DDGS
search_query = 't-shirts superman design'
results_text = DDGS().text(search_query, max_results=5)
print(results_text)
results_img = DDGS().images(search_query, max_results=5)
print(results_img)