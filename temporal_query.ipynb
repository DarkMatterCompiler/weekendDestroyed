{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoModel,RobertaTokenizer,RobertaForSequenceClassification\n",
    "import spacy\n",
    "import faiss\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.02s/it]\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at textattack/roberta-base-MNLI were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the transformer model and tokenizer for embedding (MiniLM for semantic search)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "# Load the LLaMA model and tokenizer for generating responses\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\").to(device)\n",
    "\n",
    "classifier_tokenizer = RobertaTokenizer.from_pretrained(\"textattack/roberta-base-MNLI\")\n",
    "classifier_model = RobertaForSequenceClassification.from_pretrained(\"textattack/roberta-base-MNLI\").to(device)\n",
    "classifier_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Embedding function\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(doc):\n",
    "    return {\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"author\": doc.get(\"author\", \"Unknown\"),\n",
    "        \"url\": doc.get(\"url\", \"N/A\"),\n",
    "        \"source\": doc.get(\"source\", \"N/A\"),\n",
    "        \"category\": doc.get(\"category\", \"General\"),\n",
    "        \"published_at\": doc.get(\"published_at\", \"N/A\"),\n",
    "        \"body\": doc[\"body\"]\n",
    "    }\n",
    "\n",
    "# Load corpus and process it as before\n",
    "with open(r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\corpus.json') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "corpus = [extract_metadata(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = []\n",
    "for doc in corpus:\n",
    "    body_text = doc.get('body', '')\n",
    "    try:\n",
    "        # Preprocess and tokenize the document's body\n",
    "        tokenized_doc = word_tokenize(preprocess_text(body_text))\n",
    "        tokenized_corpus.append(tokenized_doc)\n",
    "    except LookupError as e:\n",
    "        print(f\"Tokenization failed for document: {doc['title']}, Error: {e}\")\n",
    "\n",
    "# Initialize BM25 with the tokenized corpus\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bm25(query, top_k=3):\n",
    "    \"\"\"Search using BM25 to retrieve the most relevant documents.\"\"\"\n",
    "    tokenized_query = word_tokenize(preprocess_text(query))\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top_k document indices based on BM25 scores\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    \n",
    "    return top_indices\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = []\n",
    "documents = []\n",
    "\n",
    "for doc in corpus:\n",
    "    body = doc.get('body', '')\n",
    "    preprocessed_body = preprocess_text(body)\n",
    "    \n",
    "    # Optionally chunk large documents here\n",
    "    embedding = embed_text(preprocessed_body)\n",
    "    document_embeddings.append(embedding)\n",
    "    documents.append(body)\n",
    "\n",
    "# Convert embeddings to a NumPy array for FAISS\n",
    "document_embeddings = np.array(document_embeddings)\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, top_k=3):\n",
    "    \"\"\"Search the FAISS index to retrieve the most relevant documents.\"\"\"\n",
    "    preprocessed_query = preprocess_text(query)\n",
    "    query_embedding = embed_text(preprocessed_query)\n",
    "    \n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    return indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_response(query, response):\n",
    "    \"\"\"Classify LLaMA's response as Yes/No using a pre-trained RoBERTa model based on the query.\"\"\"\n",
    "    # Preprocess the query and response together\n",
    "    combined_input = f\"Query: {query}\\nResponse: {response}\"\n",
    "    inputs = classifier_tokenizer(combined_input, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get classification logits\n",
    "        outputs = classifier_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # RoBERTa's MNLI has 3 labels: [0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"]\n",
    "    # We map 'entailment' (Yes) and 'contradiction' (No) to our Yes/No answer.\n",
    "    if predicted_class == 0:\n",
    "        return 'yes'\n",
    "    elif predicted_class == 2:\n",
    "        return 'no'\n",
    "    else:\n",
    "        return 'yes'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_llama_response(query, documents, max_input_length=1500, max_output_length=500):\n",
    "    \"\"\"Generate a structured response using LLaMA model based on the query and full document bodies.\"\"\"\n",
    "    # Prepare the context for LLaMA using full document bodies\n",
    "    context = f\"Query: {query}\\n\\nRelevant information:\\n\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        context += f\"Document {i+1}: {doc['body']}\\n\\n\"\n",
    "\n",
    "    # Clearer prompt asking for yes/no answer\n",
    "    prompt = f\"\"\"{context}\n",
    "Based on the information provided, please answer the following:\n",
    "1. Can the query be answered with a 'Yes' or 'No'? Please choose either 'Yes' or 'No'.\n",
    "2. Justify your answer, citing specific evidence from the documents.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Encode and truncate the input\n",
    "    input_ids = llama_tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Calculate the maximum new tokens to generate\n",
    "    max_new_tokens = max(50, max_output_length - input_ids.shape[1])  # Ensure at least 50 new tokens\n",
    "\n",
    "    # Generate response using LLaMA\n",
    "    output = llama_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=llama_tokenizer.eos_token_id\n",
    "    )\n",
    "    llama_response = llama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "\n",
    "    answer = classify_response(query,llama_response)\n",
    "\n",
    "    # Prepare evidence list\n",
    "    evidence_list = []\n",
    "    for doc in documents:\n",
    "        evidence_list.append({\n",
    "            \"title\": doc['title'],\n",
    "            \"author\": doc.get('author', 'Unknown'),\n",
    "            \"url\": doc.get('url', 'N/A'),\n",
    "            \"source\": doc.get('source', 'N/A'),\n",
    "            \"category\": doc.get('category', 'General'),\n",
    "            \"published_at\": doc.get('published_at', 'N/A'),\n",
    "        })\n",
    "\n",
    "    # Construct the final response\n",
    "    response = {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,  # Answer provided by the classifier\n",
    "        \"evidence_list\": evidence_list    }\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_rag(query):\n",
    "\n",
    "    top_indices_bm25 = search_bm25(query, top_k=2)\n",
    "    top_docs_bm25 = [corpus[idx] for idx in top_indices_bm25]\n",
    "\n",
    "    top_documents = top_docs_bm25  # or top_docs_faiss, or a combination\n",
    "    llama_response = generate_llama_response(query, top_documents)\n",
    "    print(llama_response)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"Between the TechCrunch report on Google's approach to deepfake election risks and the subsequent TechCrunch report on a news publisher filing an antitrust suit against Google, was there a change in the portrayal of Google's impact on the industry?\", 'answer': 'yes', 'evidence_list': [{'title': 'News publisher files class action antitrust suit against Google, citing AI’s harms to their bottom line', 'author': 'Sarah Perez', 'url': 'https://techcrunch.com/2023/12/15/news-publisher-files-class-action-antitrust-suit-against-google-citing-ais-harms-to-their-bottom-line/', 'source': 'TechCrunch', 'category': 'technology', 'published_at': '2023-12-15T17:56:02+00:00'}, {'title': 'Deepfake election risks trigger EU call for more generative AI safeguards', 'author': 'Natasha Lomas', 'url': 'https://techcrunch.com/2023/09/26/generative-ai-disinformation-risks/', 'source': 'TechCrunch', 'category': 'technology', 'published_at': '2023-09-26T17:26:57+00:00'}]}\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your query: \")\n",
    "temporal_rag(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
