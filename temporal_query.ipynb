{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoModel,RobertaTokenizer,RobertaForSequenceClassification\n",
    "import spacy\n",
    "import faiss\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.98s/it]\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at textattack/roberta-base-MNLI were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the transformer model and tokenizer for embedding (MiniLM for semantic search)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "# Load the LLaMA model and tokenizer for generating responses\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\").to(device)\n",
    "\n",
    "classifier_tokenizer = RobertaTokenizer.from_pretrained(\"textattack/roberta-base-MNLI\")\n",
    "classifier_model = RobertaForSequenceClassification.from_pretrained(\"textattack/roberta-base-MNLI\").to(device)\n",
    "classifier_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Embedding function\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(doc):\n",
    "    return {\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"author\": doc.get(\"author\", \"Unknown\"),\n",
    "        \"url\": doc.get(\"url\", \"N/A\"),\n",
    "        \"source\": doc.get(\"source\", \"N/A\"),\n",
    "        \"category\": doc.get(\"category\", \"General\"),\n",
    "        \"published_at\": doc.get(\"published_at\", \"N/A\"),\n",
    "        \"body\": doc[\"body\"]\n",
    "    }\n",
    "\n",
    "# Load corpus and process it as before\n",
    "with open(r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\corpus.json') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "corpus = [extract_metadata(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = []\n",
    "for doc in corpus:\n",
    "    body_text = doc.get('body', '')\n",
    "    try:\n",
    "        # Preprocess and tokenize the document's body\n",
    "        tokenized_doc = word_tokenize(preprocess_text(body_text))\n",
    "        tokenized_corpus.append(tokenized_doc)\n",
    "    except LookupError as e:\n",
    "        print(f\"Tokenization failed for document: {doc['title']}, Error: {e}\")\n",
    "\n",
    "# Initialize BM25 with the tokenized corpus\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bm25(query, top_k=3):\n",
    "    \"\"\"Search using BM25 to retrieve the most relevant documents.\"\"\"\n",
    "    tokenized_query = word_tokenize(preprocess_text(query))\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top_k document indices based on BM25 scores\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    \n",
    "    return top_indices\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = []\n",
    "documents = []\n",
    "\n",
    "for doc in corpus:\n",
    "    body = doc.get('body', '')\n",
    "    preprocessed_body = preprocess_text(body)\n",
    "    \n",
    "    # Optionally chunk large documents here\n",
    "    embedding = embed_text(preprocessed_body)\n",
    "    document_embeddings.append(embedding)\n",
    "    documents.append(body)\n",
    "\n",
    "# Convert embeddings to a NumPy array for FAISS\n",
    "document_embeddings = np.array(document_embeddings)\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_response(query, response):\n",
    "    \"\"\"Classify LLaMA's response as Yes/No using a pre-trained RoBERTa model based on the query.\"\"\"\n",
    "    # Preprocess the query and response together\n",
    "    combined_input = f\"Query: {query}\\nResponse: {response}\"\n",
    "    inputs = classifier_tokenizer(combined_input, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get classification logits\n",
    "        outputs = classifier_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # RoBERTa's MNLI has 3 labels: [0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"]\n",
    "    # We map 'entailment' (Yes) and 'contradiction' (No) to our Yes/No answer.\n",
    "    if predicted_class == 0:\n",
    "        return 'yes'\n",
    "    elif predicted_class == 2:\n",
    "        return 'no'\n",
    "    else:\n",
    "        return 'yes'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_llama_response(query, documents, max_input_length=1500, max_output_length=500):\n",
    "    \"\"\"Generate a structured response using LLaMA model based on the query and full document bodies.\"\"\"\n",
    "    # Prepare the context for LLaMA using full document bodies\n",
    "    context = f\"Query: {query}\\n\\nRelevant information:\\n\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        context += f\"Document {i+1}: {doc['body']}\\n\\n\"\n",
    "\n",
    "    # Clearer prompt asking for yes/no answer\n",
    "    prompt = f\"\"\"{context}\n",
    "Based on the information provided, please answer the following:\n",
    "1. Can the query be answered with a 'Yes' or 'No'? Please choose either 'Yes' or 'No'.\n",
    "2. Justify your answer, citing specific evidence from the documents.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Encode and truncate the input\n",
    "    input_ids = llama_tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Calculate the maximum new tokens to generate\n",
    "    max_new_tokens = max(50, max_output_length - input_ids.shape[1])  # Ensure at least 50 new tokens\n",
    "\n",
    "    # Generate response using LLaMA\n",
    "    output = llama_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=llama_tokenizer.eos_token_id\n",
    "    )\n",
    "    llama_response = llama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "\n",
    "    answer = classify_response(query,llama_response)\n",
    "\n",
    "    # Prepare evidence list\n",
    "    evidence_list = []\n",
    "    for doc in documents:\n",
    "        evidence_list.append({\n",
    "            \"title\": doc['title'],\n",
    "            \"author\": doc.get('author', 'Unknown'),\n",
    "            \"url\": doc.get('url', 'N/A'),\n",
    "            \"source\": doc.get('source', 'N/A'),\n",
    "            \"category\": doc.get('category', 'General'),\n",
    "            \"published_at\": doc.get('published_at', 'N/A'),\n",
    "        })\n",
    "\n",
    "    # Construct the final response\n",
    "    response = {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,  # Answer provided by the classifier\n",
    "        \"evidence_list\": evidence_list    }\n",
    "\n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_rag(query):\n",
    "\n",
    "    top_indices_bm25 = search_bm25(query, top_k=2)\n",
    "    top_docs_bm25 = [corpus[idx] for idx in top_indices_bm25]\n",
    "\n",
    "    top_documents = top_docs_bm25  # or top_docs_faiss, or a combination\n",
    "    llama_response = generate_llama_response(query, top_documents)\n",
    "    return llama_response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"Between the TechCrunch report on Google's approach to deepfake election risks and the subsequent TechCrunch report on a news publisher filing an antitrust suit against Google, was there a change in the portrayal of Google's impact on the industry?\", 'answer': 'yes', 'evidence_list': [{'title': 'News publisher files class action antitrust suit against Google, citing AI’s harms to their bottom line', 'author': 'Sarah Perez', 'url': 'https://techcrunch.com/2023/12/15/news-publisher-files-class-action-antitrust-suit-against-google-citing-ais-harms-to-their-bottom-line/', 'source': 'TechCrunch', 'category': 'technology', 'published_at': '2023-12-15T17:56:02+00:00'}, {'title': 'Deepfake election risks trigger EU call for more generative AI safeguards', 'author': 'Natasha Lomas', 'url': 'https://techcrunch.com/2023/09/26/generative-ai-disinformation-risks/', 'source': 'TechCrunch', 'category': 'technology', 'published_at': '2023-09-26T17:26:57+00:00'}]}\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your query: \")\n",
    "final=temporal_rag(query)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.115.2)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.31.1)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi) (0.39.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi) (4.12.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from starlette<0.41.0,>=0.37.2->fastapi) (4.6.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.3.1)\n",
      "Requirement already satisfied: fastapi in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.115.2)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.31.1)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (1.6.0)\n",
      "Requirement already satisfied: pyngrok in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (7.2.0)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi) (0.39.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi) (4.12.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from starlette<0.41.0,>=0.37.2->fastapi) (4.6.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.3.1)\n",
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "changed 22 packages in 2m\n",
      "\n",
      "3 packages are looking for funding\n",
      "  run `npm fund` for details\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn\n",
    "!pip install fastapi uvicorn nest_asyncio pyngrok\n",
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2024-10-15T01:57:12+0530 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
      "t=2024-10-15T01:57:14+0530 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
      "t=2024-10-15T01:57:14+0530 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
      "t=2024-10-15T01:57:14+0530 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
      "t=2024-10-15T01:57:14+0530 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context canceled\"\n",
      "t=2024-10-15T01:57:14+0530 lvl=crit msg=\"command failed\" err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
     ]
    },
    {
     "ename": "PyngrokNgrokError",
     "evalue": "The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m ngrok\u001b[38;5;241m.\u001b[39mset_auth_token(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2nRVBltZDABCb3fymBI3X64f1Sa_2dp9yURb1hB6iM9BSCztw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Open a tunnel to the FastAPI app running on port 8000\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m public_url \u001b[38;5;241m=\u001b[39m \u001b[43mngrok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastAPI is publicly available at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublic_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyngrok\\ngrok.py:316\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[0;32m    312\u001b[0m             options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic_auth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [auth]\n\u001b[0;32m    314\u001b[0m         options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 316\u001b[0m api_url \u001b[38;5;241m=\u001b[39m \u001b[43mget_ngrok_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapi_url\n\u001b[0;32m    318\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating tunnel with options: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m tunnel \u001b[38;5;241m=\u001b[39m NgrokTunnel(api_request(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/tunnels\u001b[39m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    321\u001b[0m                                  timeout\u001b[38;5;241m=\u001b[39mpyngrok_config\u001b[38;5;241m.\u001b[39mrequest_timeout),\n\u001b[0;32m    322\u001b[0m                      pyngrok_config, api_url)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyngrok\\ngrok.py:156\u001b[0m, in \u001b[0;36mget_ngrok_process\u001b[1;34m(pyngrok_config)\u001b[0m\n\u001b[0;32m    152\u001b[0m     pyngrok_config \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39mget_default()\n\u001b[0;32m    154\u001b[0m install_ngrok(pyngrok_config)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyngrok\\process.py:235\u001b[0m, in \u001b[0;36mget_process\u001b[1;34m(pyngrok_config)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_process_running(pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _current_processes[pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_start_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyngrok\\process.py:398\u001b[0m, in \u001b[0;36m_start_process\u001b[1;34m(pyngrok_config)\u001b[0m\n\u001b[0;32m    395\u001b[0m kill_process(pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrok_process\u001b[38;5;241m.\u001b[39mstartup_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ngrok process errored on start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngrok_process\u001b[38;5;241m.\u001b[39mstartup_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    399\u001b[0m                             ngrok_process\u001b[38;5;241m.\u001b[39mlogs,\n\u001b[0;32m    400\u001b[0m                             ngrok_process\u001b[38;5;241m.\u001b[39mstartup_error)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ngrok process was unable to start.\u001b[39m\u001b[38;5;124m\"\u001b[39m, ngrok_process\u001b[38;5;241m.\u001b[39mlogs)\n",
      "\u001b[1;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n."
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Set the Ngrok authentication token\n",
    "ngrok.set_auth_token(\"2nRVBltZDABCb3fymBI3X64f1Sa_2dp9yURb1hB6iM9BSCztw\")\n",
    "\n",
    "# Open a tunnel to the FastAPI app running on port 8000\n",
    "public_url = ngrok.connect(8000)\n",
    "\n",
    "print(f\"FastAPI is publicly available at: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [17896]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     110.224.76.13:0 - \"POST /rag_pipeline HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Apply nest_asyncio to allow running uvicorn inside the notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define the FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define a Pydantic model for the input query\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.post(\"/rag_pipeline\")\n",
    "async def rag_pipeline_endpoint(request: QueryRequest):\n",
    "    query = request.query\n",
    "    final_answer = temporal_rag(query)  # Assuming rag_pipeline function is defined elsewhere\n",
    "    return {\"final_answer\": final_answer}\n",
    "\n",
    "# Run the FastAPI app\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
